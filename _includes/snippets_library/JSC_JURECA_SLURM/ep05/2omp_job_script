#!/bin/bash -x
#SBATCH --nodes=1
#SBATCH --output=mpi-out.%j
#SBATCH --error=mpi-err.%j
#SBATCH --time=00:05:00
#SBATCH --partition=batch

# Make sure that the multiplying the following 2 gives ncpus per node (24)
#SBATCH --ntasks-per-node=12
#SBATCH --cpus-per-task=2

# Also need to export the number of OpenMP threads so the application knows about it
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

# To switch off OpenMP threading uncomment the next line
# unset OMP_NUM_THREADS

module purge
module use /usr/local/software/jureca/OtherStages
module load Stages/Devel-2019a
module load intel-para/2019a
module load LAMMPS/9Jan2020-cuda

# srun handles the MPI placement based on the choices in the job script file
srun lmp -in in.rhodo -sf omp -pk omp $OMP_NUM_THREADS
